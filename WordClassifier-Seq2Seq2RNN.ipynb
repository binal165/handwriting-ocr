{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Word Classification\n",
    "## Using Special model\n",
    "Implemented in TensorFlow. Using Seq2Seq to generate the sequence of letter images and recognise them by RNN.\n",
    "## TODO\n",
    "```\n",
    "OPTIMIZERS\n",
    "Training options: complete, only letters, only words\n",
    "Overlaping sliders\n",
    "One-shot preprocessing\n",
    "CNN preprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.ops import math_ops\n",
    "import time\n",
    "import math\n",
    "import unidecode\n",
    "\n",
    "from ocr.datahelpers import loadWordsData, correspondingShuffle\n",
    "from ocr.mlhelpers import TrainingPlot\n",
    "from ocr.tfhelpers import Graph, create_cell\n",
    "from ocr.imgtransform import coordinates_remap\n",
    "\n",
    "%matplotlib notebook\n",
    "# Increase size of images\n",
    "plt.rcParams['figure.figsize'] = (9.0, 5.0)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "print('Tensorflow', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      "-> Number of words: 1380\n"
     ]
    }
   ],
   "source": [
    "images, labels = loadWordsData(['data/words/', 'data/words_nolines/'],\n",
    "                               loadGaplines=False)\n",
    "\n",
    "if LANG == 'en':\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = unidecode.unidecode(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars: 52\n"
     ]
    }
   ],
   "source": [
    "CHARS = ['A', 'a', 'Á', 'á', 'B', 'b', 'C', 'c', 'Č', 'č',\n",
    "         'D', 'd', 'Ď', 'ď', 'E', 'e', 'É', 'é', 'Ě', 'ě',\n",
    "         'F', 'f', 'G', 'g', 'H', 'h', 'I', 'i', 'Í', 'í',         \n",
    "         'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n',\n",
    "         'Ň', 'ň', 'O', 'o', 'Ó', 'ó', 'P', 'p', 'Q', 'q',\n",
    "         'R', 'r', 'Ř', 'ř', 'S', 's', 'Š', 'š', 'T', 't',\n",
    "         'Ť', 'ť', 'U', 'u', 'Ú', 'ú', 'Ů', 'ů', 'V', 'v',\n",
    "         'W', 'w', 'X', 'x', 'Y', 'y', 'Ý', 'ý', 'Z', 'z',\n",
    "         'Ž', 'ž']\n",
    "if LANG == 'en':\n",
    "    CHARS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
    "             'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
    "             'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c',\n",
    "             'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "             'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
    "             'x', 'y', 'z']\n",
    "    \n",
    "\n",
    "print(\"Number of chars:\", len(CHARS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0   # Padding\n",
    "EOS = 1   # End of seq\n",
    "LETTER_PAD = -1.0\n",
    "\n",
    "num_new_images = 8                #  Number of new images per image\n",
    "fac_alpha = 2.0                    # Factors for image preprocessing\n",
    "fac_sigma = 0.08\n",
    "\n",
    "num_buckets = 5\n",
    "slider_size = (60, 15)\n",
    "N_INPUT = 60*15                    # Size of sequence input vector\n",
    "char_size = len(CHARS)\n",
    "letter_size = 64*64\n",
    "\n",
    "encoder_layers = 2\n",
    "encoder_residual_layers = 1        # HAVE TO be smaller than encoder_layers\n",
    "encoder_units = 64\n",
    "\n",
    "decoder_layers = 2*encoder_layers  # 2* is due to the bidirectional encoder\n",
    "decoder_residual_layers = 2*encoder_residual_layers\n",
    "decoder_units = encoder_units\n",
    "\n",
    "wordRNN_layers = 4\n",
    "wordRNN_residual_layers = 2\n",
    "wordRNN_units = 128\n",
    "\n",
    "attention_size = 256\n",
    "\n",
    "add_output_length = 4\n",
    "\n",
    "learning_rate = 1e-4               # 1e-4\n",
    "max_gradient_norm = 5.0            # For gradient clipping\n",
    "dropout = 0.4\n",
    "train_per = 0.8                    # Percentage of training data\n",
    "\n",
    "TRAIN_STEPS = 100000               # Number of training steps!\n",
    "TEST_ITER = 150\n",
    "LOSS_ITER = 50\n",
    "SAVE_ITER = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 2000                       # Number of batches in epoch - not accurate\n",
    "save_location = 'models/word-clas/' + LANG + '/SeqRNN/Classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 1104\n",
      "Testing images: 276\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data for later splitting\n",
    "images, labels = correspondingShuffle(images, labels)\n",
    "\n",
    "idxs = [i+2 for i in range(char_size)]\n",
    "idx_to_chars = dict(zip(idxs, CHARS))\n",
    "chars_to_idx = dict(zip(CHARS, idxs))\n",
    "\n",
    "labels_idx = np.empty(len(labels), dtype=object)\n",
    "for i, label in enumerate(labels):\n",
    "    labels_idx[i] = [chars_to_idx[c] for c in label]\n",
    "\n",
    "# Split data on train and test dataset\n",
    "div = int(train_per * len(images))\n",
    "\n",
    "trainImages = images[0:div]\n",
    "testImages = images[div:]\n",
    "\n",
    "trainLabels_idx = labels_idx[0:div]\n",
    "testLabels_idx = labels_idx[div:]\n",
    "\n",
    "print(\"Training images:\", div)\n",
    "print(\"Testing images:\", len(images) - div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BucketDataIterator():\n",
    "    \"\"\" Iterator for feeding seq2seq model during training \"\"\"\n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 targets,\n",
    "                 num_buckets=5,\n",
    "                 slider=(60, 30),\n",
    "                 imgprocess=lambda x: x,\n",
    "                 train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        # First PADDING of images to slider size ( -(a // b) ==  ceil(a/b))\n",
    "        self.slider = slider\n",
    "        for i in images:\n",
    "            i.resize((i.shape[0], -(-i.shape[1] // slider[1]) * slider[1]),\n",
    "                     refcheck=False)\n",
    "        in_length = [image.shape[1]//slider[1] for image in images]\n",
    "        \n",
    "        # Split images to sequence of vectors\n",
    "        imgseq = np.empty(len(images), dtype=object)\n",
    "        for i, img in enumerate(images):\n",
    "            imgseq[i] = [imgprocess(img[:, loc * slider[1]: (loc+1) * slider[1]].flatten())\n",
    "                         for loc in range(in_length[i])]\n",
    "\n",
    "        # Create pandas dataFrame and sort it by images width (length) \n",
    "        self.dataFrame = pd.DataFrame({'in_length': in_length,\n",
    "                                       'out_length': [len(t) for t in targets],\n",
    "                                       'images': imgseq,\n",
    "                                       'targets': targets\n",
    "                                      }).sort_values('in_length').reset_index(drop=True)\n",
    "\n",
    "        bsize = int(len(images) / num_buckets)\n",
    "        self.num_buckets = num_buckets\n",
    "        \n",
    "        # Create buckets by slicing parts by indexes\n",
    "        self.buckets = []\n",
    "        for bucket in range(num_buckets-1):\n",
    "            self.buckets.append(self.dataFrame.iloc[bucket * bsize: (bucket+1) * bsize])\n",
    "        self.buckets.append(self.dataFrame.iloc[(num_buckets-1) * bsize:])        \n",
    "        \n",
    "        self.buckets_size = [len(bucket) for bucket in self.buckets]\n",
    "\n",
    "        # cursor[i] will be the cursor for the ith bucket\n",
    "        self.cursor = np.array([0] * num_buckets)\n",
    "        self.bucket_order = np.random.permutation(num_buckets)\n",
    "        self.bucket_cursor = 0\n",
    "        self.shuffle()\n",
    "        print(\"Iterator created.\")\n",
    "\n",
    "    def shuffle(self, idx=None):\n",
    "        \"\"\" Shuffle idx bucket or each bucket separately \"\"\"\n",
    "        for i in [idx] if idx is not None else range(self.num_buckets):\n",
    "            self.buckets[i] = self.buckets[i].sample(frac=1).reset_index(drop=True)\n",
    "            self.cursor[i] = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Creates next training batch of size: batch_size\n",
    "        Retruns: image seq, letter seq,\n",
    "                 image seq lengths, letter seq lengths\n",
    "        \"\"\"\n",
    "        i_bucket = self.bucket_order[self.bucket_cursor]\n",
    "        # Increment cursor and shuffle in case of new round\n",
    "        self.bucket_cursor = (self.bucket_cursor + 1) % self.num_buckets\n",
    "        if self.bucket_cursor == 0:\n",
    "            self.bucket_order = np.random.permutation(self.num_buckets)\n",
    "            \n",
    "        if self.cursor[i_bucket] + batch_size > self.buckets_size[i_bucket]:\n",
    "            self.shuffle(i_bucket)\n",
    "\n",
    "        # Handle too big batch sizes\n",
    "        if (batch_size > self.buckets_size[i_bucket]):\n",
    "            batch_size = self.buckets_size[i_bucket]\n",
    "\n",
    "        res = self.buckets[i_bucket].iloc[self.cursor[i_bucket]:\n",
    "                                          self.cursor[i_bucket]+batch_size]\n",
    "        self.cursor[i_bucket] += batch_size\n",
    "\n",
    "        # PAD input sequence and output\n",
    "        # Pad sequences with <PAD> to same length\n",
    "        input_max = max(res['in_length'])\n",
    "        output_max = max(res['out_length'])\n",
    "        # In order to make it work at production\n",
    "        assert np.all(res['in_length'] + add_output_length >= res['out_length'])\n",
    "        \n",
    "        input_seq = np.zeros((batch_size, input_max, N_INPUT), dtype=np.float32)\n",
    "        for i, img in enumerate(res['images']):\n",
    "            input_seq[i][:res['in_length'].values[i]] = img\n",
    "        input_seq = input_seq.swapaxes(0, 1)\n",
    "        \n",
    "        # Need to pad according to the maximum length output sequence\n",
    "        targets = np.zeros([batch_size, output_max], dtype=np.int32)\n",
    "        for i, target in enumerate(targets):\n",
    "            target[:res['out_length'].values[i]] = res['targets'].values[i]\n",
    "        targets = targets.swapaxes(0, 1)\n",
    "        \n",
    "        return input_seq, targets, res['in_length'].values, res['out_length'].values\n",
    "    \n",
    "    def next_feed(self, size):\n",
    "        \"\"\" Create feed directly for model training \"\"\"\n",
    "        (encoder_inputs_,\n",
    "         decoder_targets_,\n",
    "         encoder_inputs_length_,\n",
    "         decoder_targets_length_) = self.next_batch(size)\n",
    "        return {\n",
    "            encoder_inputs: encoder_inputs_,\n",
    "            encoder_inputs_length: encoder_inputs_length_,\n",
    "            word_targets: decoder_targets_,\n",
    "            word_targets_length: decoder_targets_length_,\n",
    "            keep_prob: (1.0 - dropout) if self.train else 1.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created.\n",
      "Iterator created.\n"
     ]
    }
   ],
   "source": [
    "# Create iterator for feeding RNN\n",
    "# Create only once, it modifies: labels_idx\n",
    "train_iterator = BucketDataIterator(trainImages,\n",
    "                                    trainLabels_idx,\n",
    "                                    num_buckets,\n",
    "                                    slider_size,\n",
    "                                    train=True)\n",
    "test_iterator = BucketDataIterator(testImages,\n",
    "                                   testLabels_idx,\n",
    "                                   num_buckets,\n",
    "                                   slider_size,\n",
    "                                   train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input placehodlers\n",
    "# Only encoder inputs are time major\n",
    "# N_INPUT -> size of vector representing one image in sequence\n",
    "# Encoder inputs shape (max_seq_length, batch_size, vec_size)\n",
    "encoder_inputs = tf.placeholder(shape=(None, None, N_INPUT),\n",
    "                                dtype=tf.float32,\n",
    "                                name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                       dtype=tf.int32,\n",
    "                                       name='encoder_inputs_length')\n",
    "\n",
    "# Required for letter sep. training\n",
    "# Contains EOS symbol\n",
    "letter_targets = tf.placeholder(shape=(None, None, letter_size),\n",
    "                                dtype=tf.float32,\n",
    "                                name='letter_targets')\n",
    "# Without EOS, need append START symbol\n",
    "letter_train_inputs = tf.placeholder(shape=(None, None, letter_size),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='letter_train_inputs')\n",
    "letter_targets_length = tf.placeholder(shape=(None,),\n",
    "                                       dtype=tf.int32,\n",
    "                                       name='letter_targets_length')\n",
    "\n",
    "# Required for word training\n",
    "word_targets = tf.placeholder(shape=(None, None),\n",
    "                              dtype=tf.int32,\n",
    "                              name='word_targets')\n",
    "word_targets_length = tf.placeholder(shape=(None,),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='word_targets_length')\n",
    "# Dropout value\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "# Testing control\n",
    "is_training = tf.placeholder(tf.bool, shape=None, name=\"is_training\")\n",
    "is_words = tf.placeholder(tf.bool, shape=None, name=\"is_words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Train Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_size, batch_size, _ = tf.unstack(tf.shape(encoder_inputs)) # letter_targets\n",
    "\n",
    "EOS_SLICE = tf.cast(tf.fill([batch_size, 1, letter_size], EOS), tf.float32)\n",
    "PAD_SLICE = tf.cast(tf.fill([batch_size, 1, letter_size], PAD), tf.float32)\n",
    "\n",
    "# Train inputs with EOS symbol at start of seq\n",
    "letter_train_inputs = tf.concat([PAD_SLICE, letter_train_inputs], axis=1) #EOS_SLICE\n",
    "letter_train_length = letter_targets_length\n",
    "\n",
    "# Length of infer (test) letter output\n",
    "# TODO: will have to make shorte...\n",
    "output_length = tf.reduce_max(encoder_inputs_length) + add_output_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_cell_fw = create_cell(encoder_units,\n",
    "                          encoder_layers,\n",
    "                          encoder_residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)\n",
    "enc_cell_bw = create_cell(encoder_units,\n",
    "                          encoder_layers,\n",
    "                          encoder_residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = encoder_inputs\n",
    "\n",
    "# Bidirectional RNN, gibe fw and bw outputs separately\n",
    "enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw = enc_cell_fw,\n",
    "    cell_bw = enc_cell_bw,\n",
    "    inputs = inputs,\n",
    "    sequence_length = encoder_inputs_length,\n",
    "    dtype = tf.float32,\n",
    "    time_major = True)\n",
    "\n",
    "encoder_outputs = tf.concat(enc_outputs, -1)\n",
    "\n",
    "if encoder_layers == 1:\n",
    "    encoder_state = enc_state\n",
    "else:\n",
    "    encoder_state = []\n",
    "    for layer_id in range(encoder_layers):\n",
    "        encoder_state.append(enc_state[0][layer_id])  # forward\n",
    "        encoder_state.append(enc_state[1][layer_id])  # backward\n",
    "    encoder_state = tuple(encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attention_states: size [batch_size, max_time, num_units]\n",
    "attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = seq2seq.LuongAttention(\n",
    "    decoder_units, attention_states,\n",
    "    memory_sequence_length=encoder_inputs_length)\n",
    "\n",
    "decoder_cell = create_cell(decoder_units,\n",
    "                           decoder_layers,\n",
    "                           decoder_residual_layers,\n",
    "                           is_dropout=True,\n",
    "                           keep_prob=keep_prob)\n",
    "\n",
    "decoder_cell = seq2seq.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism,\n",
    "    attention_layer_size=attention_size)\n",
    "\n",
    "decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(\n",
    "    cell_state=encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "helper = seq2seq.TrainingHelper(\n",
    "    letter_train_inputs, letter_targets_length)\n",
    "\n",
    "# Decoder\n",
    "projection_layer = layers_core.Dense(\n",
    "    letter_size, use_bias=False)\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "# Dynamic decoding\n",
    "outputs, final_context_state, _ = seq2seq.dynamic_decode(\n",
    "    decoder)\n",
    "\n",
    "letter_logits_train = outputs.rnn_output\n",
    "letter_prediction_train = outputs.sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INFERENCE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper without embedding, can add param: 'next_inputs_fn'\n",
    "helper_infer = seq2seq.InferenceHelper(\n",
    "    sample_fn=(lambda x: x),\n",
    "    sample_shape=[letter_size],\n",
    "    sample_dtype=tf.float32,\n",
    "    start_inputs=tf.cast(tf.fill([batch_size, letter_size], PAD), tf.float32), # PAD <- EOS, need flaot32\n",
    "    end_fn=(lambda sample_ids: math_ops.equal(tf.argmax(sample_ids[:, :2]), 1))) # ID - is image or not, first 2 pixels\n",
    "\n",
    "decoder_infer = seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper_infer, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "# Dynamic decoding\n",
    "outputs_infer, final_context_state, final_seq_lengths = seq2seq.dynamic_decode(\n",
    "    decoder_infer,\n",
    "    impute_finished=True,\n",
    "    maximum_iterations=output_length)\n",
    "\n",
    "letter_prediction_infer = tf.identity(outputs_infer.sample_id,\n",
    "                                      name='letter_prediction_infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [2] vs. [4]\n\t [[Node: decoder_1/while/LogicalOr = LogicalOr[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder_1/while/BasicDecoderStep/Equal, decoder_1/while/Identity_15)]]\n\nCaused by op 'decoder_1/while/LogicalOr', defined at:\n  File \"/home/breta/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/breta/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0d349a2358f0>\", line 17, in <module>\n    maximum_iterations=output_length)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 235, in body\n    next_finished = math_ops.logical_or(decoder_finished, finished)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2387, in logical_or\n    \"LogicalOr\", x=x, y=y, name=name)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [4]\n\t [[Node: decoder_1/while/LogicalOr = LogicalOr[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder_1/while/BasicDecoderStep/Equal, decoder_1/while/Identity_15)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2] vs. [4]\n\t [[Node: decoder_1/while/LogicalOr = LogicalOr[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder_1/while/BasicDecoderStep/Equal, decoder_1/while/Identity_15)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-40e67f605006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletter_prediction_infer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2] vs. [4]\n\t [[Node: decoder_1/while/LogicalOr = LogicalOr[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder_1/while/BasicDecoderStep/Equal, decoder_1/while/Identity_15)]]\n\nCaused by op 'decoder_1/while/LogicalOr', defined at:\n  File \"/home/breta/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/breta/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0d349a2358f0>\", line 17, in <module>\n    maximum_iterations=output_length)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 235, in body\n    next_finished = math_ops.logical_or(decoder_finished, finished)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2387, in logical_or\n    \"LogicalOr\", x=x, y=y, name=name)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/breta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [4]\n\t [[Node: decoder_1/while/LogicalOr = LogicalOr[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder_1/while/BasicDecoderStep/Equal, decoder_1/while/Identity_15)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "fd  = test_iterator.next_feed(4)\n",
    "pre = letter_prediction_infer.eval(fd)\n",
    "print(pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_RNN = create_cell(wordRNN_units,\n",
    "                       wordRNN_layers,\n",
    "                       wordRNN_residual_layers,\n",
    "                       is_dropout=True,\n",
    "                       keep_prob=keep_prob)\n",
    "\n",
    "word_inputs = tf.cond(is_training,\n",
    "                      lambda: letter_targets,\n",
    "                      lambda: letter_prediction_infer)\n",
    "word_inputs_length = tf.cond(is_training,\n",
    "                             lambda: letter_targets_length,\n",
    "                             lambda: final_seq_lengths)\n",
    "\n",
    "# Word RNN\n",
    "word_outputs, _ = tf.nn.dynamic_rnn(\n",
    "    cell = cell_RNN,\n",
    "    inputs = word_inputs,\n",
    "    sequence_length = word_inputs_length,\n",
    "    dtype = tf.float32)\n",
    "\n",
    "pred = tf.layers.dense(inputs=word_outputs,\n",
    "                       units=char_size,\n",
    "                       name='pred')\n",
    "word_prediction = tf.argmax(pred, axis=-1, name='word_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights + Paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad test accuracy\n",
    "letter_test_targets = tf.pad(\n",
    "    letter_targets,\n",
    "    [[0, 0],\n",
    "     [0, output_length - tf.reduce_max(letter_targets_length)],\n",
    "     [0, 0]],\n",
    "    constant_values=LETTER_PAD,\n",
    "    mode='CONSTANT')\n",
    "\n",
    "# Pad prediction to match lengths\n",
    "letter_pred_infer_pad = tf.pad(\n",
    "    letter_prediction_infer,\n",
    "    [[0, 0],\n",
    "     [0, output_length - tf.reduce_max(word_inputs_length)],\n",
    "     [0, 0]],\n",
    "    constant_values=LETTER_PAD,\n",
    "    mode='CONSTANT')\n",
    "\n",
    "word_pad_lenght = tf.maximum(\n",
    "    tf.reduce_max(word_inputs_length),\n",
    "    tf.reduce_max(word_targets_length))\n",
    "\n",
    "word_pred_pad = tf.pad(\n",
    "    letter_prediction_infer,\n",
    "    [[0, 0],\n",
    "     [0, word_pad_lenght - tf.reduce_max(word_inputs_length)]],\n",
    "    constant_values=PAD,\n",
    "    mode='CONSTANT')\n",
    "word_targets_pad = tf.pad(\n",
    "    letter_prediction_infer,\n",
    "    [[0, 0],\n",
    "     [0, word_pad_lenght - tf.reduce_max(word_targets_length)]],\n",
    "    constant_values=PAD,\n",
    "    mode='CONSTANT')\n",
    "\n",
    "\n",
    "# Weights\n",
    "letter_loss_weights = tf.sequence_mask(\n",
    "    letter_train_length,\n",
    "    tf.reduce_max(letter_train_length),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "letter_test_weights = tf.sequence_mask(\n",
    "    letter_train_length,\n",
    "    output_length,\n",
    "    dtype=tf.float32)\n",
    "\n",
    "word_loss_weights = tf.sequence_mask(\n",
    "    word_pad_lenght,\n",
    "    tf.reduce_max(word_targets_length),\n",
    "    dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_train_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9cf622e42226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_train_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_test_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m loss = seq2seq.sequence_loss(logits=letter_logits_train,\n\u001b[1;32m      5\u001b[0m                              \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mletter_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_train_targets' is not defined"
     ]
    }
   ],
   "source": [
    "## Loss\n",
    "letter_loss = seq2seq.sequence_loss(\n",
    "    logits=letter_logits_train,\n",
    "    targets=letter_targets,\n",
    "    weights=letter_loss_weights,\n",
    "    name='loss')\n",
    "\n",
    "word_loss = seq2seq.sequence_loss(\n",
    "    logits=word_pred_pad,\n",
    "    targets=word_targets_pad,\n",
    "    weights=word_loss_weights,\n",
    "    name='loss')\n",
    "\n",
    "loss = tf.cond(is_words,\n",
    "               lambda: word_loss,\n",
    "               lambda: letter_loss)\n",
    "\n",
    "## Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "\n",
    "### Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params),\n",
    "    name='train_step')\n",
    "\n",
    "# TODO: Accuracy controled by is_words\n",
    "\n",
    "### Evaluate model\n",
    "correct_prediction = tf.equal(\n",
    "    letter_pred_infer_pad,\n",
    "    letter_test_targets)\n",
    "## Advanced accuracy only the elements of seq including EOS symbol\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32)*test_weights)/tf.reduce_sum(test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Creat plot for live stats ploting\n",
    "trainPlot = TrainingPlot(TRAIN_STEPS, TEST_ITER, LOSS_ITER)\n",
    "\n",
    "try:\n",
    "    for i_batch in range(TRAIN_STEPS):\n",
    "        # DEBUG\n",
    "        break\n",
    "        \n",
    "        fd = train_iterator.next_feed(BATCH_SIZE)\n",
    "        train_step.run(fd)\n",
    "        \n",
    "        if i_batch % LOSS_ITER == 0:\n",
    "            # Plotting loss\n",
    "            tmpLoss = loss.eval(fd)\n",
    "            trainPlot.updateCost(tmpLoss, i_batch // LOSS_ITER)\n",
    "    \n",
    "        if i_batch % TEST_ITER == 0:\n",
    "            # Plotting accuracy\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            accTest = accuracy.eval(fd_test)\n",
    "            accTrain = accuracy.eval(fd)\n",
    "            trainPlot.updateAcc(accTest, accTrain, i_batch // TEST_ITER)\n",
    "\n",
    "        if i_batch % SAVE_ITER == 0:\n",
    "            saver.save(sess, save_location)\n",
    "        \n",
    "        if i_batch % EPOCH == 0:\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            print('batch %r - loss: %r' % (i_batch, sess.run(loss, fd_test)))\n",
    "            predict_, target_ = sess.run([prediction_infer_padded, test_targets], fd_test)\n",
    "            for i, (inp, pred) in enumerate(zip(target_, predict_)):\n",
    "                print('    expected  > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 1:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    saver.save(sess, save_location)\n",
    "    print('Training interrupted, model saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
