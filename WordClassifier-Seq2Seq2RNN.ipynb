{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Word Classification\n",
    "## Using Special model\n",
    "Implemented in TensorFlow. Using Seq2Seq to generate the sequence of letter images and recognise them by RNN.\n",
    "## TODO\n",
    "```\n",
    "OPTIMIZERS\n",
    "Training options: complete, only letters, only words\n",
    "Overlaping sliders\n",
    "One-shot preprocessing\n",
    "CNN preprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.ops import math_ops\n",
    "import time\n",
    "import math\n",
    "import unidecode\n",
    "\n",
    "from ocr.datahelpers import loadWordsData, correspondingShuffle\n",
    "from ocr.mlhelpers import TrainingPlot\n",
    "from ocr.tfhelpers import Graph, create_cell\n",
    "from ocr.imgtransform import coordinates_remap\n",
    "\n",
    "%matplotlib notebook\n",
    "# Increase size of images\n",
    "plt.rcParams['figure.figsize'] = (9.0, 5.0)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "print('Tensorflow', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      "-> Number of words: 1380\n"
     ]
    }
   ],
   "source": [
    "images, labels = loadWordsData(['data/words/', 'data/words_nolines/'],\n",
    "                               loadGaplines=False)\n",
    "\n",
    "if LANG == 'en':\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = unidecode.unidecode(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars: 52\n"
     ]
    }
   ],
   "source": [
    "CHARS = ['A', 'a', 'Á', 'á', 'B', 'b', 'C', 'c', 'Č', 'č',\n",
    "         'D', 'd', 'Ď', 'ď', 'E', 'e', 'É', 'é', 'Ě', 'ě',\n",
    "         'F', 'f', 'G', 'g', 'H', 'h', 'I', 'i', 'Í', 'í',         \n",
    "         'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n',\n",
    "         'Ň', 'ň', 'O', 'o', 'Ó', 'ó', 'P', 'p', 'Q', 'q',\n",
    "         'R', 'r', 'Ř', 'ř', 'S', 's', 'Š', 'š', 'T', 't',\n",
    "         'Ť', 'ť', 'U', 'u', 'Ú', 'ú', 'Ů', 'ů', 'V', 'v',\n",
    "         'W', 'w', 'X', 'x', 'Y', 'y', 'Ý', 'ý', 'Z', 'z',\n",
    "         'Ž', 'ž']\n",
    "if LANG == 'en':\n",
    "    CHARS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
    "             'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
    "             'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c',\n",
    "             'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "             'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
    "             'x', 'y', 'z']\n",
    "    \n",
    "\n",
    "print(\"Number of chars:\", len(CHARS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0   # Padding\n",
    "EOS = 1   # End of seq\n",
    "\n",
    "num_new_images = 8                #  Number of new images per image\n",
    "fac_alpha = 2.0                    # Factors for image preprocessing\n",
    "fac_sigma = 0.08\n",
    "\n",
    "num_buckets = 5\n",
    "slider_size = (60, 15)\n",
    "N_INPUT = 60*15                    # Size of sequence input vector\n",
    "char_size = len(CHARS)\n",
    "letter_size = 64*64\n",
    "\n",
    "encoder_layers = 2\n",
    "encoder_residual_layers = 1        # HAVE TO be smaller than encoder_layers\n",
    "encoder_units = 64\n",
    "\n",
    "decoder_layers = 2*encoder_layers  # 2* is due to the bidirectional encoder\n",
    "decoder_residual_layers = 2*encoder_residual_layers\n",
    "decoder_units = encoder_units\n",
    "\n",
    "wordRNN_layers = 4\n",
    "wordRNN_residual_layers = 2\n",
    "wordRNN_units = 128\n",
    "\n",
    "attention_size = 256\n",
    "\n",
    "add_output_length = 4\n",
    "\n",
    "learning_rate = 1e-4               # 1e-4\n",
    "max_gradient_norm = 5.0            # For gradient clipping\n",
    "dropout = 0.4\n",
    "train_per = 0.8                    # Percentage of training data\n",
    "\n",
    "TRAIN_STEPS = 100000               # Number of training steps!\n",
    "TEST_ITER = 150\n",
    "LOSS_ITER = 50\n",
    "SAVE_ITER = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 2000                       # Number of batches in epoch - not accurate\n",
    "save_location = 'models/word-clas/' + LANG + '/SeqRNN/WordClassifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 1104\n",
      "Testing images: 276\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data for later splitting\n",
    "images, labels = correspondingShuffle(images, labels)\n",
    "\n",
    "idxs = [i+2 for i in range(char_size)]\n",
    "idx_to_chars = dict(zip(idxs, CHARS))\n",
    "chars_to_idx = dict(zip(CHARS, idxs))\n",
    "\n",
    "labels_idx = np.empty(len(labels), dtype=object)\n",
    "for i, label in enumerate(labels):\n",
    "    labels_idx[i] = [chars_to_idx[c] for c in label]\n",
    "\n",
    "# Split data on train and test dataset\n",
    "div = int(train_per * len(images))\n",
    "\n",
    "trainImages = images[0:div]\n",
    "testImages = images[div:]\n",
    "\n",
    "trainLabels_idx = labels_idx[0:div]\n",
    "testLabels_idx = labels_idx[div:]\n",
    "\n",
    "print(\"Training images:\", div)\n",
    "print(\"Testing images:\", len(images) - div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BucketDataIterator():\n",
    "    \"\"\" Iterator for feeding seq2seq model during training \"\"\"\n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 targets,\n",
    "                 num_buckets=5,\n",
    "                 slider=(60, 30),\n",
    "                 imgprocess=lambda x: x,\n",
    "                 train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        # First PADDING of images to slider size ( -(a // b) ==  ceil(a/b))\n",
    "        self.slider = slider\n",
    "        for i in images:\n",
    "            i.resize((i.shape[0], -(-i.shape[1] // slider[1]) * slider[1]),\n",
    "                     refcheck=False)\n",
    "        in_length = [image.shape[1]//slider[1] for image in images]\n",
    "        \n",
    "        # Split images to sequence of vectors\n",
    "        imgseq = np.empty(len(images), dtype=object)\n",
    "        for i, img in enumerate(images):\n",
    "            imgseq[i] = [imgprocess(img[:, loc * slider[1]: (loc+1) * slider[1]].flatten())\n",
    "                         for loc in range(in_length[i])]\n",
    "\n",
    "        # Create pandas dataFrame and sort it by images width (length) \n",
    "        self.dataFrame = pd.DataFrame({'in_length': in_length,\n",
    "                                       'out_length': [len(t) for t in targets],\n",
    "                                       'images': imgseq,\n",
    "                                       'targets': targets\n",
    "                                      }).sort_values('in_length').reset_index(drop=True)\n",
    "\n",
    "        bsize = int(len(images) / num_buckets)\n",
    "        self.num_buckets = num_buckets\n",
    "        \n",
    "        # Create buckets by slicing parts by indexes\n",
    "        self.buckets = []\n",
    "        for bucket in range(num_buckets-1):\n",
    "            self.buckets.append(self.dataFrame.iloc[bucket * bsize: (bucket+1) * bsize])\n",
    "        self.buckets.append(self.dataFrame.iloc[(num_buckets-1) * bsize:])        \n",
    "        \n",
    "        self.buckets_size = [len(bucket) for bucket in self.buckets]\n",
    "\n",
    "        # cursor[i] will be the cursor for the ith bucket\n",
    "        self.cursor = np.array([0] * num_buckets)\n",
    "        self.bucket_order = np.random.permutation(num_buckets)\n",
    "        self.bucket_cursor = 0\n",
    "        self.shuffle()\n",
    "        print(\"Iterator created.\")\n",
    "\n",
    "    def shuffle(self, idx=None):\n",
    "        \"\"\" Shuffle idx bucket or each bucket separately \"\"\"\n",
    "        for i in [idx] if idx is not None else range(self.num_buckets):\n",
    "            self.buckets[i] = self.buckets[i].sample(frac=1).reset_index(drop=True)\n",
    "            self.cursor[i] = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Creates next training batch of size: batch_size\n",
    "        Retruns: image seq, letter seq,\n",
    "                 image seq lengths, letter seq lengths\n",
    "        \"\"\"\n",
    "        i_bucket = self.bucket_order[self.bucket_cursor]\n",
    "        # Increment cursor and shuffle in case of new round\n",
    "        self.bucket_cursor = (self.bucket_cursor + 1) % self.num_buckets\n",
    "        if self.bucket_cursor == 0:\n",
    "            self.bucket_order = np.random.permutation(self.num_buckets)\n",
    "            \n",
    "        if self.cursor[i_bucket] + batch_size > self.buckets_size[i_bucket]:\n",
    "            self.shuffle(i_bucket)\n",
    "\n",
    "        # Handle too big batch sizes\n",
    "        if (batch_size > self.buckets_size[i_bucket]):\n",
    "            batch_size = self.buckets_size[i_bucket]\n",
    "\n",
    "        res = self.buckets[i_bucket].iloc[self.cursor[i_bucket]:\n",
    "                                          self.cursor[i_bucket]+batch_size]\n",
    "        self.cursor[i_bucket] += batch_size\n",
    "\n",
    "        # PAD input sequence and output\n",
    "        # Pad sequences with <PAD> to same length\n",
    "        input_max = max(res['in_length'])\n",
    "        output_max = max(res['out_length'])\n",
    "        # In order to make it work at production\n",
    "        assert np.all(res['in_length'] + add_output_length >= res['out_length'])\n",
    "        \n",
    "        input_seq = np.zeros((batch_size, input_max, N_INPUT), dtype=np.float32)\n",
    "        for i, img in enumerate(res['images']):\n",
    "            input_seq[i][:res['in_length'].values[i]] = img\n",
    "        input_seq = input_seq.swapaxes(0, 1)\n",
    "        \n",
    "        # Need to pad according to the maximum length output sequence\n",
    "        targets = np.zeros([batch_size, output_max], dtype=np.int32)\n",
    "        for i, target in enumerate(targets):\n",
    "            target[:res['out_length'].values[i]] = res['targets'].values[i]\n",
    "        targets = targets.swapaxes(0, 1)\n",
    "        \n",
    "        return input_seq, targets, res['in_length'].values, res['out_length'].values\n",
    "    \n",
    "    def next_feed(self, size):\n",
    "        \"\"\" Create feed directly for model training \"\"\"\n",
    "        (encoder_inputs_,\n",
    "         decoder_targets_,\n",
    "         encoder_inputs_length_,\n",
    "         decoder_targets_length_) = self.next_batch(size)\n",
    "        return {\n",
    "            encoder_inputs: encoder_inputs_,\n",
    "            encoder_inputs_length: encoder_inputs_length_,\n",
    "            word_targets: decoder_targets_,\n",
    "            word_targets_length: decoder_targets_length_,\n",
    "            keep_prob: (1.0 - dropout) if self.train else 1.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created.\n",
      "Iterator created.\n"
     ]
    }
   ],
   "source": [
    "# Create iterator for feeding RNN\n",
    "# Create only once, it modifies: labels_idx\n",
    "train_iterator = BucketDataIterator(trainImages,\n",
    "                                    trainLabels_idx,\n",
    "                                    num_buckets,\n",
    "                                    slider_size,\n",
    "                                    train=True)\n",
    "test_iterator = BucketDataIterator(testImages,\n",
    "                                   testLabels_idx,\n",
    "                                   num_buckets,\n",
    "                                   slider_size,\n",
    "                                   train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input placehodlers\n",
    "# N_INPUT -> size of vector representing one image in sequence\n",
    "# Encoder inputs shape (max_seq_length, batch_size, vec_size)\n",
    "encoder_inputs = tf.placeholder(shape=(None, None, N_INPUT),\n",
    "                                dtype=tf.float32,\n",
    "                                name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                       dtype=tf.int32,\n",
    "                                       name='encoder_inputs_length')\n",
    "\n",
    "# Required for letter sep. training\n",
    "letter_targets = tf.placeholder(shape=(None, None),\n",
    "                                dtype=tf.int32,\n",
    "                                name='letter_targets')\n",
    "letter_targets_length = tf.placeholder(shape=(None,),\n",
    "                                       dtype=tf.int32,\n",
    "                                       name='letter_targets_length')\n",
    "\n",
    "# Required for word training\n",
    "word_targets = tf.placeholder(shape=(None, None),\n",
    "                              dtype=tf.int32,\n",
    "                              name='word_targets')\n",
    "word_targets_length = tf.placeholder(shape=(None,),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='word_targets_length')\n",
    "# Dropout value\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Train Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_size, batch_size = tf.unstack(tf.shape(word_targets))\n",
    "\n",
    "EOS_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * EOS\n",
    "PAD_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * PAD\n",
    "\n",
    "# Train inputs with EOS symbol at start of seq\n",
    "decoder_train_inputs = tf.concat([EOS_SLICE, word_targets], axis=0)\n",
    "decoder_train_length = word_targets_length + 1\n",
    "\n",
    "# train targets with EOS symbol at end of seq\n",
    "decoder_train_targets = tf.concat([word_targets, PAD_SLICE], axis=0)\n",
    "decoder_train_targets_seq_len, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "decoder_train_targets_eos_mask = tf.one_hot(decoder_train_length - 1,\n",
    "                                            decoder_train_targets_seq_len,\n",
    "                                            on_value=EOS, off_value=PAD,\n",
    "                                            dtype=tf.int32)\n",
    "decoder_train_targets_eos_mask = tf.transpose(decoder_train_targets_eos_mask, [1, 0])\n",
    "\n",
    "# hacky way using one_hot to put EOS symbol at the end of target sequence\n",
    "decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                               decoder_train_targets_eos_mask)\n",
    "\n",
    "# Pad test accuracy\n",
    "decoder_test_targets = tf.pad(\n",
    "    decoder_train_targets,\n",
    "    [[0, tf.reduce_max(encoder_inputs_length) + add_output_length - decoder_train_targets_seq_len], [0, 0]],\n",
    "    mode='CONSTANT')\n",
    "\n",
    "loss_weights = tf.sequence_mask(\n",
    "    decoder_train_length,\n",
    "    tf.reduce_max(decoder_train_length),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "test_weights = tf.sequence_mask(\n",
    "    decoder_train_length,\n",
    "    tf.reduce_max(encoder_inputs_length) + add_output_length,\n",
    "    dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_cell_fw = create_cell(encoder_units,\n",
    "                          encoder_layers,\n",
    "                          encoder_residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)\n",
    "enc_cell_bw = create_cell(encoder_units,\n",
    "                          encoder_layers,\n",
    "                          encoder_residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = encoder_inputs\n",
    "\n",
    "# Bidirectional RNN, gibe fw and bw outputs separately\n",
    "enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw = enc_cell_fw,\n",
    "    cell_bw = enc_cell_bw,\n",
    "    inputs = inputs,\n",
    "    sequence_length = encoder_inputs_length,\n",
    "    dtype = tf.float32,\n",
    "    time_major = True)\n",
    "\n",
    "encoder_outputs = tf.concat(enc_outputs, -1)\n",
    "\n",
    "if encoder_layers == 1:\n",
    "    encoder_state = enc_state\n",
    "else:\n",
    "    encoder_state = []\n",
    "    for layer_id in range(encoder_layers):\n",
    "        encoder_state.append(enc_state[0][layer_id])  # forward\n",
    "        encoder_state.append(enc_state[1][layer_id])  # backward\n",
    "    encoder_state = tuple(encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attention_states: size [batch_size, max_time, num_units]\n",
    "attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = seq2seq.LuongAttention(\n",
    "    decoder_units, attention_states,\n",
    "    memory_sequence_length=encoder_inputs_length)\n",
    "\n",
    "decoder_cell = create_cell(decoder_units,\n",
    "                           decoder_layers,\n",
    "                           decoder_residual_layers,\n",
    "                           is_dropout=True,\n",
    "                           keep_prob=keep_prob)\n",
    "\n",
    "decoder_cell = seq2seq.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism,\n",
    "    attention_layer_size=attention_size)\n",
    "\n",
    "decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(\n",
    "    cell_state=encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_train_inputs_embedded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5de7213d4b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m helper = seq2seq.TrainingHelper(\n\u001b[0;32m----> 3\u001b[0;31m     decoder_train_inputs_embedded, decoder_train_length, time_major=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_train_inputs_embedded' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper\n",
    "helper = seq2seq.TrainingHelper(\n",
    "    decoder_train_inputs_embedded, decoder_train_length, time_major=True)\n",
    "\n",
    "# Decoder\n",
    "projection_layer = layers_core.Dense(\n",
    "    letter_size, use_bias=False)\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "# Dynamic decoding\n",
    "outputs, final_context_state, _ = seq2seq.dynamic_decode(\n",
    "    decoder)\n",
    "\n",
    "logits_train = outputs.rnn_output\n",
    "prediction_train = outputs.sample_id\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INFERENCE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper without embedding, can add param: 'next_inputs_fn'\n",
    "helper_infer2 = seq2seq.InferenceHelper(\n",
    "    sample_fn=(lambda x: x),\n",
    "    sample_shape=[letter_size],\n",
    "    sample_dtype=tf.float32,\n",
    "    start_inputs=tf.cast(\n",
    "        tf.fill([batch_size, letter_size], PAD), tf.float32), # PAD <- EOS\n",
    "    end_fn=(lambda sample_ids: math_ops.greater(sample_ids[:, -1], 0)))\n",
    "\n",
    "# Decoder\n",
    "projection_layer2 = layers_core.Dense(\n",
    "    letter_size, use_bias=False)\n",
    "\n",
    "decoder_infer2 = seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper_infer2, decoder_initial_state,\n",
    "    output_layer=projection_layer2)\n",
    "\n",
    "# Dynamic decoding\n",
    "outputs_infer, final_context_state, final_seq_lengths = seq2seq.dynamic_decode(\n",
    "    decoder_infer2,\n",
    "    impute_finished=True,\n",
    "    maximum_iterations=tf.reduce_max(encoder_inputs_length) + add_output_length)\n",
    "prediction_inference = tf.identity(outputs_infer.sample_id,\n",
    "                                   name='prediction_infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "fd  = test_iterator.next_feed(2)\n",
    "pre = prediction_inference.eval(fd)\n",
    "print(pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_RNN = create_cell(wordRNN_units,\n",
    "                       wordRNN_layers,\n",
    "                       wordRNN_residual_layers,\n",
    "                       is_dropout=True,\n",
    "                       keep_prob=keep_prob)\n",
    "\n",
    "# Word RNN\n",
    "word_outputs, _ = tf.nn.dynamic_rnn(\n",
    "    cell = cell_RNN,\n",
    "    inputs = prediction_inference,\n",
    "    sequence_length = final_seq_lengths,\n",
    "    dtype = tf.float32)\n",
    "\n",
    "pred = tf.layers.dense(inputs=word_outputs,\n",
    "                       units=char_size,\n",
    "                       name='pred')\n",
    "prediction = tf.argmax(pred, axis=-1, name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = tf.transpose(decoder_train_targets, [1, 0])\n",
    "test_targets = tf.transpose(decoder_test_targets, [1, 0])\n",
    "## Loss\n",
    "loss = seq2seq.sequence_loss(logits=logits_train,\n",
    "                             targets=targets,\n",
    "                             weights=loss_weights,\n",
    "                             name='loss')\n",
    "\n",
    "## Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "\n",
    "### Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params),\n",
    "    name='train_step')\n",
    "\n",
    "\n",
    "### Evaluate model\n",
    "# Pad prediction to match lengths\n",
    "prediction_infer_padded = tf.pad(\n",
    "    prediction_inference,\n",
    "    [[0, 0], [0, tf.reduce_max(encoder_inputs_length) + add_output_length - tf.reduce_max(final_seq_lengths)]],\n",
    "    mode='CONSTANT')\n",
    "\n",
    "correct_prediction = tf.equal(prediction_infer_padded,\n",
    "                              test_targets)\n",
    "## Advanced accuracy only the elements of seq including EOS symbol\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32)*test_weights)/tf.reduce_sum(test_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Creat plot for live stats ploting\n",
    "trainPlot = TrainingPlot(TRAIN_STEPS, TEST_ITER, LOSS_ITER)\n",
    "\n",
    "try:\n",
    "    for i_batch in range(TRAIN_STEPS):\n",
    "        # DEBUG\n",
    "        break\n",
    "        \n",
    "        fd = train_iterator.next_feed(BATCH_SIZE)\n",
    "        train_step.run(fd)\n",
    "        \n",
    "        if i_batch % LOSS_ITER == 0:\n",
    "            # Plotting loss\n",
    "            tmpLoss = loss.eval(fd)\n",
    "            trainPlot.updateCost(tmpLoss, i_batch // LOSS_ITER)\n",
    "    \n",
    "        if i_batch % TEST_ITER == 0:\n",
    "            # Plotting accuracy\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            accTest = accuracy.eval(fd_test)\n",
    "            accTrain = accuracy.eval(fd)\n",
    "            trainPlot.updateAcc(accTest, accTrain, i_batch // TEST_ITER)\n",
    "\n",
    "        if i_batch % SAVE_ITER == 0:\n",
    "            saver.save(sess, save_location)\n",
    "        \n",
    "        if i_batch % EPOCH == 0:\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            print('batch %r - loss: %r' % (i_batch, sess.run(loss, fd_test)))\n",
    "            predict_, target_ = sess.run([prediction_infer_padded, test_targets], fd_test)\n",
    "            for i, (inp, pred) in enumerate(zip(target_, predict_)):\n",
    "                print('    expected  > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 1:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    saver.save(sess, save_location)\n",
    "    print('Training interrupted, model saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
